# 语言识别（Auto Speech Recognition）

## 特征提取

孤立词语识别：假如现在只要识别两个词，yes 和 no，得到的是两个语音波形，现在要
对两个语音进行进行特征提取。

为什么要进行特征提取？因为直接处理语音的波形是很不方便的，特征提取是为了将语音
波形转换成好比较的向量。

帧的概念：语音信号其实是一个时间序列。每个单词发音都是由音素组成，每个音素的发音大概是100Hz，也就是差不多
周期为10ms，语音中一帧通常要包括多个周期,所以帧通常取20-50ms。可以对一段语音取
多个帧，比如每10ms移动，然后取一帧。比如一段1s长的语音信号，可以转换成100帧。

然后对每一帧信号进行处理，因为每一帧都是时间序列，可以对该时间序列进行傅立叶变
换，将这100帧傅立叶变换的图，拼接成一个矩阵，就变成了语谱图。相当于STFT。傅立
叶变换后，时域变成了频域，然后做三角滤波，去除掉信号的精细结构，获得整个频域信
号的包络，然后对三角滤波的输出做离散余弦变换（DCT）后，将每一帧时间信号，转换
成了固定长度的向量，这个向量叫做MFCC（Mel frequency cepstral coefficients）
,MFCC可以表示这一帧大部分信息。

到此，就完成了语音信号的特征提取，1s信号有100帧，可以转换成一个100\*length 长
的MFCC序列

## GMM 高斯混合模型

假如我的训练集语料库中有很多个yes的序列，测试集中也有yes的序列，如何进行匹配呢
？训练集中的语料yes，可以转换成一个模型，这个模型就是高斯混合模型，因为每个时
间序列yes，都转变成了MFCC序列矩阵，不同yes发音，可以得到多个MFCC序列矩阵。因为
yes中MFCC向量有相似的，所以不同yes发音，但是相似的向量，会合并成一个状态。比如
y发音，算一个状态，e发音也是一个状态，每个状态中有很多MFCC向量。

这个模型是怎么训练出来的？高斯混合模型是由多个高斯模型混合而成的，假设每个MFCC
向量是13维度，那么这个向量其实是13维空间中的一个点。这些MFCC点，在13维空间中是
有个分布的，发音相似的状态向量，点的分布是非常相近的。假如新来了一个yes发音，
将他转换成很多MFCC向量后，计算每个向量和模型中相似状态的概率分布，然后将这些概
率相乘，就得到这个发音是yes的概率。

对其的概念，注意每个单词要于模型中相似的单词向量进行对其，不然不好计算概率。


## 评价指标

词错误率，计算方法如下：
- 将标准答案与识别结果对齐
- 因为识别肯定会识别错单词，或者某些单词没识别到，所以要统计插入，删除，替换的
  错误总数，除以标准答案的长度
- 对齐使错误的数量最少（动态规划）

但上面评价指标，没考虑单词的错误程度。
