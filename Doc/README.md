# 语言识别（Auto Speech Recognition）

## 常见术语介绍

- [kaldi 中文手册](https://shiweipku.gitbooks.io/chinese-doc-of-kaldi/content/) 

**语音的基本保存方式**：

语音本来是模拟信号，通过模数转换成波形图(waveform),波形图是一个离散的时间序列，由很多个电压数值表
示。如：1s的音频可以用16000个电压值表示，则采样频率为16kHz

**什么是ASR**？

语音识别属于序列转换技术，将语音序列转换成文本序列。

**语音识别基本思路**:

1. 已知一段语音序列，1s的序列，将序列表示为很多帧，通常每20ms
2. 将语音序列处理成声学特征向量$X=[x_1, x_2, x_3,...]$,其中$x_i$表示一帧的特征
   向量，可能的文本序列表示为$w=[w_1, w_2, w_3,...]$，我们要求
   $w^*=argmax_wp(w|x)$,表示x声学特征向量能够很好的表示这句话，即使得这句话出
   现的概率最大。

$$
P(W|X) = \frac{P(X|W)P(W)}{P(X)}
\propto
P(X|W)P(W)
$$

其中$P(X|W)$称作声学模型，$P(W)$称为语言模型，传统的ASR设计都是设计这两个模型
，后来基于深度学习和大数据的端到端发展起来，直接计算$P(W|X)$, 把声学模型和语言
模型融为一体。

> 要将语音变成文本，第一想到就是使用一个函数，这个函数的输入是语音信号，输出是
> 文本。但是即使是同一句话，不同人读，语音信号也不一样，定义域和值域都非常大，
> 如果能找到语音和文本两者的基本组成单位，那么二者的映射关系会单纯一些。

> 语音选择的基本单位是：帧，每帧固定维度不变，一帧形式就是一个向量，整条语音可
> 以整理为以帧为单位的向量组，一帧数据由一小段语音经过ASR前端声学特征提取模块
> 产生，涉及的技术包括离散傅立叶变换和滤波器组。

> 文本的选择基本单位有很多方式：字词，音素，三音素等

**传统的ASR**:

传统的声学模型无非是

- 特征提取
- 声学模型(AM)
- 语言模型(LM)
- 有时候还会有音素模型(PM)

特征提取：[MFCC](http://www.hzhcontrols.com/new-940426.html) 就是将时间语音信号，转换成固定长度的特征向量，语音信号分为很多
帧，这里对每一帧都可以做MFCC,最后MFCC输出的是一个数组，其中每个元素都是定长的向量 :D

**什么是声学模型**：主要是将一段语音转换成音素或者拼音 其中涉及到[HMM(Hidden Markov Model)](https://zhuanlan.zhihu.com/p/224770895)

**什么是语言模型**：主要对声学模型得到的拼音进行猜测，得到文本，也就是计算这几个词组成一
个句子的概率有多高，选概率最高的那个.
![Picture](https://pic1.zhimg.com/v2-f6a3c10ef3f8d6780abf53fb660571b0_b.webp)

**什么是字典**：词典就是发音字典的意思，中文中就是拼音与汉字的对应，英文中就是音标与单词的对应，其目的是根据声学模型识别出来的音素，来找到对应的汉字（词）或者单词，用来在声学模型和语言模型建立桥梁，将两者联系起来。

**端到端结构**:

端到端结构就是把传统ASR结构中的声学模型替代了。:)

即输入是语音信号(每一帧信号为基本单位)输出是对应的文本(以音素或字为基本单位)

端到端首先需要解决的问题是输入输出不定长问题：

对于输入：可以考虑将不同长度的数据转换成固定维度的向量序列，如果输入声学模型的
语音图，可以使用CNN进行特征提取，转化成固定长的序列。如果输入以帧为单位，可以
使用RNN，可以将积累的历史信息在最后以固定维度一次性输出。

对于输出：在语音识别中，输出长度一般要远小于输入长度，可以引入空白标签充数，这
里要引入[CTC(Connectionist Temporal Classification)](https://zhuanlan.zhihu.com/p/36488476?spm=a2c6h.12873639.0.0.78641e800SrLpI) 损失函数常用技巧，输出长度可
以用其他机制判断是否结束，比如引入结束符标签，当输出该标签时表示结束。

**CTC**:仅仅是一种损失函数，即输入是一个序列，输出也是一个序列。该损失函数使得
模型输出序列尽可能拟合目标序列。即优化函数$w^*=argmax_wp(w|x)$,表示x声学特征向量能够很好的表示这句话，即使得这句话出
现的概率最大。

与传统的声学模型训练相比，采用CTC作为损失函数的声学模型训练，是一种完全端到端的声学模型训练，不需要预先对数据做对齐，只需要一个输入序列和一个输出序列即可以训练。这样就不需要对数据对齐和一一标注，并且CTC直接输出序列预测的概率，不需要外部的后处理。:)

- [CTC REFERENCE1](https://xiaodu.io/ctc-explained/) 
- [CTC REFERENCE2](https://xiaodu.io/ctc-explained-part2/) 

**RNN-T**:也是一种损失函数，相对于CTC的不同，RNN-T在训练的时候，会用到之前的预
测结果。而CTC不会。

除此之外，还有attention，self-attention.


## 特征提取

孤立词语识别：假如现在只要识别两个词，yes 和 no，得到的是两个语音波形，现在要
对两个语音进行进行特征提取。

为什么要进行特征提取？因为直接处理语音的波形是很不方便的，特征提取是为了将语音
波形转换成好比较的向量。

帧的概念：语音信号其实是一个时间序列。每个单词发音都是由音素组成，每个音素的发音大概是100Hz，也就是差不多
周期为10ms，语音中一帧通常要包括多个周期,所以帧通常取20-50ms。可以对一段语音取
多个帧，比如每10ms移动，然后取一帧。比如一段1s长的语音信号，可以转换成100帧。

然后对每一帧信号进行处理，因为每一帧都是时间序列，可以对该时间序列进行傅立叶变
换，将这100帧傅立叶变换的图，拼接成一个矩阵，就变成了语谱图。相当于STFT。傅立
叶变换后，时域变成了频域，然后做三角滤波，去除掉信号的精细结构，获得整个频域信
号的包络，然后对三角滤波的输出做离散余弦变换（DCT）后，将每一帧时间信号，转换
成了固定长度的向量，这个向量叫做MFCC（Mel frequency cepstral coefficients）
,MFCC可以表示这一帧大部分信息。

到此，就完成了语音信号的特征提取，1s信号有100帧，可以转换成一个100\*length 长
的MFCC序列

## GMM 高斯混合模型

假如我的训练集语料库中有很多个yes的序列，测试集中也有yes的序列，如何进行匹配呢
？训练集中的语料yes，可以转换成一个模型，这个模型就是高斯混合模型，因为每个时
间序列yes，都转变成了MFCC序列矩阵，不同yes发音，可以得到多个MFCC序列矩阵。因为
yes中MFCC向量有相似的，所以不同yes发音，但是相似的向量，会合并成一个状态。比如
y发音，算一个状态，e发音也是一个状态，每个状态中有很多MFCC向量。

这个模型是怎么训练出来的？高斯混合模型是由多个高斯模型混合而成的，假设每个MFCC
向量是13维度，那么这个向量其实是13维空间中的一个点。这些MFCC点，在13维空间中是
有个分布的，发音相似的状态向量，点的分布是非常相近的。假如新来了一个yes发音，
将他转换成很多MFCC向量后，计算每个向量和模型中相似状态的概率分布，然后将这些概
率相乘，就得到这个发音是yes的概率。

对其的概念，注意每个单词要于模型中相似的单词向量进行对其，不然不好计算概率。


## 评价指标

词错误率，计算方法如下：
- 将标准答案与识别结果对齐
- 因为识别肯定会识别错单词，或者某些单词没识别到，所以要统计插入，删除，替换的
  错误总数，除以标准答案的长度
- 对齐使错误的数量最少（动态规划）

但上面评价指标，没考虑单词的错误程度。
